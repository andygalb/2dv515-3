computer architecture a pipelined implementation of the mips architecture pipelining is a key concept in computer architecture in computer engineering computer architecture is a set of rules and methods that describe the functionality organization and implementation of computer systems some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation history the first documented computer architecture was in the correspondence between charles babbage and ada lovelace describing the analytical engine when building the computer z1 in 1936 konrad zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data ie the storedprogram concept two other early and important examples are john von neumanns 1945 paper first draft of a report on the edvac which described an organization of logical elements and alan turings more detailed proposed electronic calculator for the automatic computing engine also 1945 and which cited john von neumanns paper the term architecture in computer literature can be traced to the work of lyle r johnson and frederick p brooks jr members of the machine organization department in ibms main research center in 1959 johnson had the opportunity to write a proprietary research communication about the stretch an ibmdeveloped supercomputer for los alamos national laboratory at the time known as los alamos scientific laboratory to describe the level of detail for discussing the luxuriously embellished computer he noted that his description of formats instruction types hardware parameters and speed enhancements were at the level of system architecture a term that seemed more useful than machine organization subsequently brooks a stretch designer started chapter 2 of a book planning a computer system project stretch ed w buchholz 1962 by writing mwparseroutput templatequoteoverflowhiddenmargin1em 0padding0 40pxmwparseroutput templatequote templatequotecitelineheight15emtextalignleftpaddingleft16emmargintop0computer architecture like other architecture is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints brooks went on to help develop the ibm system360 now called the ibm zseries line of computers in which architecture became a noun defining what the user needs to know the earliest computer architectures were designed on paper and then directly built into the final hardware form later computer architecture prototypes were physically built in the form of a transistortransistor logic ttl computersuch as the prototypes of the 6800 and the parisctested and tweaked before committing to the final hardware form as of the 1990s new computer architectures are typically built tested and tweakedinside some other computer architecture in a computer architecture simulator or inside a fpga as a soft microprocessor or bothbefore committing to the final hardware form subcategories the discipline of computer architecture has three main subcategories instruction set architecture or isa the isa defines the machine code that a processor reads and acts upon as well as the word size memory address modes processor registers and data type microarchitecture or computer organization describes how a particular processor will implement the isa the size of a computers cpu cache for instance is an issue that generally has nothing to do with the isa system design includes all of the other hardware components within a computing system these include data processing other than the cpu such as direct memory access dma other issues such as virtualization multiprocessing and software features there are other types of computer architecture the following types are used in bigger companies like intel and count for 1 of all of computer architecture macroarchitecture architectural layers more abstract than microarchitecture assembly instruction set architecture isa a smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations programmer visible macroarchitecture higher level language tools such as compilers may define a consistent interface or contract to programmers using them abstracting differences between underlying isa uisa and microarchitectures eg the c c or java standards define different programmer visible macroarchitecture uisa microcode instruction set architecturea group of machines with different hardware level microarchitectures may share a common microcode architecture and hence a uisa pin architecture the hardware functions that a microprocessor should provide to a hardware platform eg the x86 pins a20m ferrignne or flush also messages that the processor should emit so that external caches can be invalidated emptied pin architecture functions are more flexible than isa functions because external hardware can adapt to new encodings or change from a pin to a message the term architecture fits because the functions must be provided for compatible systems even if the detailed method changes roles definition the purpose is to design a computer that maximizes performance while keeping power consumption in check costs low relative to the amount of expected performance and is also very reliable for this many aspects are to be considered which includes instruction set design functional organization logic design and implementation the implementation involves integrated circuit design packaging power and cooling optimization of the design requires familiarity with compilers operating systems to logic design and packaging instruction set architecture instruction set architecture an instruction set architecture isa is the interface between the computers software and hardware and also can be viewed as the programmers view of the machine computers do not understand highlevel programming languages such as java c or most programming languages used a processor only understands instructions encoded in some numerical fashion usually as binary numbers software tools such as compilers translate those high level languages into instructions that the processor can understand besides instructions the isa defines items in the computer that are available to a programeg data types registers addressing modes and memory instructions locate these available items with register indexes or names and memory addressing modes the isa of a computer is usually described in a small instruction manual which describes how the instructions are encoded also it may define short vaguely mnemonic names for the instructions the names can be recognized by a software development tool called an assembler an assembler is a computer program that translates a humanreadable form of the isa into a computerreadable form disassemblers are also widely available usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs isas vary in quality and completeness a good isa compromises between programmer convenience how easy the code is to understand size of the code how much code is required to do a specific action cost of the computer to interpret the instructions more complexity means more hardware needed to decode and execute the instructions and speed of the computer with more complex decoding hardware comes longer decode time memory organization defines how instructions interact with the memory and how memory interacts with itself during design emulation software emulators can run programs written in a proposed instruction set modern emulators can measure size cost and speed to determine if a particular isa is meeting its goals computer organization microarchitecture computer organization helps optimize performancebased products for example software engineers need to know the processing power of processors they may need to optimize software in order to gain the most performance for the lowest price this can require quite detailed analysis of the computers organization for example in a sd card the designers might need to arrange the card so that the most data can be processed in the fastest possible way computer organization also helps plan the selection of a processor for a particular project multimedia projects may need very rapid data access while virtual machines may need fast interrupts sometimes certain tasks need additional components as well for example a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated computer organization and features also affect power consumption and processor cost implementation implementation once an instruction set and microarchitecture are designed a practical machine must be developed this design process is called the implementation implementation is usually not considered architectural design but rather hardware design engineering implementation can be further broken down into several steps logic implementation designs the circuits required at a logic gate level circuit implementation does transistorlevel designs of basic elements gates multiplexers latches etc as well as of some larger blocks alus caches etc that may be implemented at the log gate level or even at the physical level if the design calls for it physical implementation draws physical circuits the different circuit components are placed in a chip floorplan or on a board and the wires connecting them are created design validation tests the computer as a whole to see if it works in all situations and all timings once the design validation process starts the design at the logic level are tested using logic emulators however this is usually too slow to run realistic test so after making corrections based on the first test prototypes are constructed using fieldprogrammable gatearrays fpgas most hobby projects stop at this stage the final step is to test prototype integrated circuits integrated circuits may require several redesigns to fix problems for cpus the entire implementation process is organized differently and is often referred to as cpu design design goals the exact form of a computer system depends on the constraints and goals computer architectures usually trade off standards power versus performance cost memory capacity latency latency is the amount of time that it takes for information from one node to travel to the source and throughput sometimes other considerations such as features size weight reliability and expandability are also factors the most common scheme does an in depth power analysis and figures out how to keep power consumption low while maintaining adequate performance performance modern computer performance is often described in ipc instructions per cycle this measures the efficiency of the architecture at any clock frequency since a faster rate can make a faster computer this is a useful measurement older computers had ipc counts as low as 01 instructions per cycle simple modern processors easily reach near 1 superscalar processors may reach three to five ipc by executing several instructions per clock cycle counting machine language instructions would be misleading because they can do varying amounts of work in different isas the instruction in the standard measurements is not a count of the isas actual machine language instructions but a unit of measurement usually based on the speed of the vax computer architecture many people used to measure a computers speed by the clock rate usually in mhz or ghz this refers to the cycles per second of the main clock of the cpu however this metric is somewhat misleading as a machine with a higher clock rate may not necessarily have greater performance as a result manufacturers have moved away from clock speed as a measure of performance other factors influence speed such as the mix of functional units bus speeds available memory and the type and order of instructions in the programs there are two main types of speed latency and throughput latency is the time between the start of a process and its completion throughput is the amount of work done per unit time interrupt latency is the guaranteed maximum response time of the system to an electronic event like when the disk drive finishes moving some data performance is affected by a very wide range of design choices for example pipelining a processor usually makes latency worse but makes throughput better computers that control machinery usually need low interrupt latencies these computers operate in a realtime environment and fail if an operation is not completed in a specified amount of time for example computercontrolled antilock brakes must begin braking within a predictable short time after the brake pedal is sensed or else failure of the brake will occur benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs although benchmarking shows strengths it shouldnt be how you choose a computer often the measured machines split on different measures for example one system might handle scientific applications quickly while another might render video games more smoothly furthermore designers may target and add special features to their products through hardware or software that permit a specific benchmark to execute quickly but dont offer similar advantages to general tasks power efficiency lowpower electronics power efficiency is another important measurement in modern computers a higher power efficiency can often be traded for lower speed or higher cost the typical measurement when referring to power consumption in computer architecture is mipsw millions of instructions per second per watt modern circuits have less power required per transistor as the number of transistors per chip grows in the world of embedded computers power efficiency has long been an important goal next to throughput and latency shifts in market demand increases in publicly released refresh rates have grown slowly over the past few years with respect to vast leaps in power consumption reduction and miniaturization demand this has led to a new demand for longer battery life and reductions in size due to the mobile technology being produced at a greater rate this change in focus from greater refresh rates to power consumption and miniaturization can be shown by the significant reductions in power consumption as much as 50 that were reported by intel in their release of the haswell microarchitecture where they dropped their power consumption benchmark from 3040 watts down to 1020 watts it can be seen that the focus in research and development are shifting away from refresh rates and moving towards consuming less power and taking up less space computer science portal electronics portal computing portal comparison of cpu architectures computer hardware cpu design floating point harvard modified dataflow tta reconfigurable computing influence of the ibm pc on the personal computer market orthogonal instruction set software architecture von neumann architecture flynns taxonomy john l hennessy and david patterson 2006 computer architecture a quantitative approach fourth ed morgan kaufmann isbn9780123704900 barton robert s functional design of computers communications of the acm 49 405 1961 barton robert s a new approach to the functional design of a digital computer proceedings of the western joint computer conference may 1961 pp393396 about the design of the burroughs b5000 computer bell c gordon and newell allen 1971 computer structures readings and examples mcgrawhill blaauw ga and brooks fp jr the structure of system360 part ioutline of the logical structure ibm systems journal vol 3 no 2 pp119135 1964 tanenbaum andrew s 1979 structured computer organization englewood cliffs new jersey prenticehall isbn0131485210 wikimedia commons has media related to computer architecture isca proceedings of the international symposium on computer architecture micro ieeeacm international symposium on microarchitecture hpca international symposium on high performance computer architecture asplos international conference on architectural support for programming languages and operating systems acm transactions on architecture and code optimization ieee transactions on computers the von neumann architecture of computer systems vtemajor fields of computer sciencenote this template roughly follows the 2012 acm computing classification systemhardware printed circuit board peripheral integrated circuit very large scale integration systems on chip socs energy consumption green computing electronic design automation hardware acceleration computer systemsorganization computer architecture embedded system realtime computing dependability networks network architecture network protocol network components network scheduler network performance evaluation network service software organization interpreter middleware virtual machine operating system software quality software notationsand tools programming paradigm programming language compiler domainspecific language modeling language software framework integrated development environment software configuration management software library software repository software development software development process requirements analysis software design software construction software deployment software maintenance programming team opensource model theory of computation model of computation formal language automata theory computational complexity theory logic semantics algorithms algorithm design analysis of algorithms algorithmic efficiency randomized algorithm computational geometry mathematicsof computing discrete mathematics probability statistics mathematical software information theory mathematical analysis numerical analysis informationsystems database management system information storage systems enterprise information system social information systems geographic information system decision support system process control system multimedia information system data mining digital library computing platform digital marketing world wide web information retrieval security cryptography formal methods security services intrusion detection system hardware security network security information security application security humancomputerinteraction interaction design social computing ubiquitous computing visualization accessibility concurrency concurrent computing parallel computing distributed computing multithreading multiprocessing artificialintelligence natural language processing knowledge representation and reasoning computer vision automated planning and scheduling search methodology control method philosophy of artificial intelligence distributed artificial intelligence machine learning supervised learning unsupervised learning reinforcement learning multitask learning crossvalidation graphics animation rendering image manipulation graphics processing unit mixed reality virtual reality image compression solid modeling appliedcomputing ecommerce enterprise software computational mathematics computational physics computational chemistry computational biology computational social science computational engineering computational healthcare digital art electronic publishing cyberwarfare electronic voting video games word processing operations research educational technology document management book category portal wikiproject commons authority control gnd 40487179 